{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMJaN0b5ekCspXVSXdmNsOJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bryanttsai96/Diamond-Price/blob/main/Diamond_(drop_variables).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wbeyS0_jT7F"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "import plotly.express as px"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel(\"UV6248-XLS-ENG.xls\", sheet_name = \"Raw Data\")\n",
        "df = df.rename(columns={'Carat Weight': 'CaratWeight'})\n",
        "df = df.dropna()\n",
        "df1= df.copy()"
      ],
      "metadata": {
        "id": "B7qjTu3ajU8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the number of observations across categories in all variables.\n",
        "\n",
        "For problematic categories, combine the ''rare categories\" together.\n",
        "\n",
        "Try this on your own with your teammates. If you do not how, ask ChatGPT."
      ],
      "metadata": {
        "id": "QQTYzuybjaaD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1.Clarity.unique()\n",
        "print(df1.Clarity.unique())\n",
        "FL_count2 = df1['Clarity'].value_counts()\n",
        "print(FL_count2)\n",
        "\n",
        "# FL is quite low and consider combine or drop it\n",
        "df1['Clarity']= df1['Clarity'].replace({'IF': 'IF_FL','FL': 'IF_FL'})\n",
        "df1.Clarity.unique()\n",
        "print(df1.Clarity.unique())"
      ],
      "metadata": {
        "id": "2Ha-4MMGjdAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2.2 Visual Exploration of the Data\n",
        "Explore impact of different categorical variables on our data."
      ],
      "metadata": {
        "id": "8aTsTB09jgIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = px.scatter(df1,\n",
        "                 x=\"CaratWeight\",\n",
        "                 y=\"Price\",\n",
        "                 color = 'Color',\n",
        "                 height = 350\n",
        "                )\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "28Kqqgh6jiFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#explore whether taking a log of X or Y helps the relationship.\n",
        "from math import log\n",
        "#by doing copy the original data won't be change by all the number being log\n",
        "df_log = df.copy()\n",
        "df_log['Price']=df_log['Price'].transform(log)\n",
        "#df_log['CaratWeight']=df_log['CaratWeight'].transform(log)\n",
        "print(df_log['Price'].head(10))\n",
        "\n",
        "fig = px.scatter(df_log,\n",
        "                 x=\"CaratWeight\",\n",
        "                 y=\"Price\",\n",
        "                 color = 'Color',\n",
        "                 height = 350,\n",
        "                 labels={'Price':'Log of Price'#, 'CaratWeight': 'Log of CaratWeight'\n",
        "                         }\n",
        "                )\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "hxm9aTHsjmJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create dummy variables for categorical variables using pd.get_dummies():"
      ],
      "metadata": {
        "id": "TSiCSv9RjrXv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = pd.get_dummies(data=df, columns = ['Color', 'Report'], drop_first=True)\n",
        "df1.head() # Check how the data looks now\n",
        "# ['Color','Clarity','Cut','Polish','Symmetry','Report']"
      ],
      "metadata": {
        "id": "dtOVOg2mjs26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Look at the correlation matrix to see if we have potential multicollinearity\n",
        "\n",
        "Encode ordinal categorical variables using an increasing scale"
      ],
      "metadata": {
        "id": "llqbhXJmjvew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scale_map_cut = {'Fair':1, 'Good':2, 'Very Good':3, 'Ideal':4,  'Signature-Ideal':5}\n",
        "scale_map_clarity = {'SI1':1, 'VVS2':2, 'VVS1':3, 'VS2':4, 'VS1':5, 'IF_FL':6}\n",
        "scale_map_polish = {'G':1, 'VG':2, 'EX':3, 'ID':3}\n",
        "\n",
        "df1[\"Cut\"] = df1[\"Cut\"].replace(scale_map_cut)\n",
        "df1[\"Clarity\"] = df1[\"Clarity\"].replace(scale_map_clarity)\n",
        "df1[\"Polish\"] = df1[\"Polish\"].replace(scale_map_polish)\n",
        "df1[\"Symmetry\"] = df1[\"Symmetry\"].replace(scale_map_polish)\n",
        "df1.tail()"
      ],
      "metadata": {
        "id": "rQXsQPCWjxIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df1.dtypes)\n",
        "df1.corr().style.background_gradient(cmap='RdBu_r', axis=None)\n",
        "df1.drop(['Price','ID'], axis=1).corr().style.background_gradient(cmap='coolwarm', axis=None)\n",
        "#\"axis=None\" option above indicates that the colors are assigned based on the values in the whole matrix\n",
        "# Other good color maps: 'RdBu_r' & 'PuOr_r' & 'coolwarm'"
      ],
      "metadata": {
        "id": "mKFHBDEgjzkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute Variance Inflation Factors (VIF)"
      ],
      "metadata": {
        "id": "mpAY3_Clj3mf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HOGk6FiLj7Fa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "# VIF dataframe\n",
        "\n",
        "vif_data1 = pd.DataFrame()\n",
        "vif_data1[\"feature\"] = df1.drop(['Price','ID'], axis=1).columns\n",
        "\n",
        "\n",
        "# calculating VIF for each feature\n",
        "vif_data1[\"VIF\"] = [variance_inflation_factor(df1.drop(['Price','ID'], axis=1).values, i)\n",
        "                          for i in range(len(df1.drop(['Price','ID'], axis=1).columns))]\n",
        "print(vif_data1)"
      ],
      "metadata": {
        "id": "GVgTcG2bj1ap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try mean-centering variables\n",
        "Try creating combinations of variables\n",
        "Try dropping variables"
      ],
      "metadata": {
        "id": "3EVI6uhVj8e-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#drop 'Symmetry','Polish', 'Cut'\n",
        "vif_data2 = pd.DataFrame()\n",
        "vif_data2[\"feature\"] = df1.drop(['Price','ID','Symmetry','Polish'], axis=1).columns\n",
        "\n",
        "\n",
        "# calculating VIF for each feature\n",
        "vif_data2[\"VIF\"] = [variance_inflation_factor(df1.drop(['Price','ID','Symmetry','Polish'], axis=1).values, i)\n",
        "                          for i in range(len(df1.drop(['Price','ID', 'Symmetry','Polish'], axis=1).columns))]\n",
        "print(vif_data2)"
      ],
      "metadata": {
        "id": "ArKXgAJwj-sY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the data into X and Y.\n",
        "The vector of Y (\"dependent\") variable should contain the Price. The matrix of X (\"independent\") variables should contain everything we will use to predict Y"
      ],
      "metadata": {
        "id": "8eMm0oLqkCBT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Y = df1[(['Price'])]\n",
        "Y.head() # it's always a good idea to peak at your output"
      ],
      "metadata": {
        "id": "hwv1MQdLkEdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df1.drop(['Price','ID', 'Report_GIA','Symmetry','Polish'], axis=1)\n",
        "#X = df[['CaratWeight']]\n",
        "X.dtypes"
      ],
      "metadata": {
        "id": "P1jHHwIQkHv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build regression models\n",
        "3.1 Try a linear model"
      ],
      "metadata": {
        "id": "87ID2psekJn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In this package, by default, the regression will have no intercept, hence we need to manually add it to the X matrix, and call the result X_const\n",
        "X_const = sm.add_constant(X)\n",
        "X_const.head()\n",
        "# Fit a linear regression model with vector Y as dependent and matrix X_sm as independent\n",
        "lm = sm.OLS(Y, X_const).fit()\n",
        "\n",
        "# Display the summary of model results\n",
        "print(lm.summary())"
      ],
      "metadata": {
        "id": "Zt6VtWZpkIZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot the residuals"
      ],
      "metadata": {
        "id": "NLYq1VlTkPvi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the residuals\n",
        "results = pd.DataFrame()\n",
        "results['Price'] = df['Price']\n",
        "results['prediction_lm'] = lm.fittedvalues\n",
        "results['residual_lm'] = lm.resid\n",
        "\n",
        "fig = px.scatter(\n",
        "    results, x='prediction_lm', y='residual_lm', height = 350,\n",
        "    labels={'prediction_lm':'Predicted values using the Linear Model', 'residual_lm':'Residuals using the Linear Model'}\n",
        ")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "i41Lo3arkRxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "log-linear model to the case data"
      ],
      "metadata": {
        "id": "x3c1it6xkWmG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from math import exp, log\n",
        "Y_log = Y['Price'].transform(log)\n",
        "log_linear_model = sm.OLS(Y_log, X_const).fit()\n",
        "print(log_linear_model.summary())"
      ],
      "metadata": {
        "id": "zzAIcF3MkXEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute residuals\n",
        "results['prediction_llm'] = log_linear_model.fittedvalues\n",
        "results['residual_llm'] = log_linear_model.resid\n",
        "\n",
        "fig = px.scatter(\n",
        "    results, x='prediction_llm', y='residual_llm', height = 350,\n",
        "    labels={'prediction_llm':'Predicted values using the Log-Linear Model', 'residual_llm':'Residuals'}\n",
        ")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "pnkbT8NZkaWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fit a log-log model to the case data"
      ],
      "metadata": {
        "id": "9btklhSMkbr3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from math import exp, log\n",
        "Y_log = Y['Price'].transform(log)\n",
        "#duplicate x for making the original data clean\n",
        "X_log= X.copy()\n",
        "X_log['CaratWeight'] = X_log['CaratWeight'].transform(log)\n",
        "X_log_const= sm.add_constant(X_log)\n",
        "X_log_const.head()\n",
        "log_log_model = sm.OLS(Y_log, X_log_const).fit()\n",
        "print(log_log_model.summary())"
      ],
      "metadata": {
        "id": "o2ACF_kdkdRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute residuals\n",
        "results['prediction_llm'] = log_log_model.fittedvalues\n",
        "results['residual_llm'] = log_log_model.resid\n",
        "\n",
        "fig = px.scatter(\n",
        "    results, x='prediction_llm', y='residual_llm', height = 350,\n",
        "    labels={'prediction_llm':'Predicted values using the Log-Log Model', 'residual_llm':'Residuals'}\n",
        ")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "0GqW4KiukgSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cross-validation\n",
        "The main machine learning principle that allows to answer this question -- cross-validation: splitting the data into training (80%) and testing (20%) subsets, training on the former and testing on the latter. Find the MAPE"
      ],
      "metadata": {
        "id": "FEqYs49gklQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Redefine X and Y for the training and testing data\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "#Add a constant to the X's:\n",
        "X_train_const = sm.add_constant(X_train)\n",
        "X_test_const = sm.add_constant(X_test)\n",
        "\n",
        "X_train_const.head()"
      ],
      "metadata": {
        "id": "u9p6QcjJknub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit a linear regression model to the training data\n",
        "lm = sm.OLS(Y_train, X_train_const).fit()\n",
        "\n",
        "# Use the trained model to predict the prices for the testing data. Call the vector of predicted prices Y_pred\n",
        "Y_pred = lm.predict(X_test_const)\n",
        "percent_errors = np.abs((Y_test['Price'] - Y_pred) / Y_test['Price']) *100\n",
        "print(\"Linear Model MAPE = \", np.mean(percent_errors), \"%\")"
      ],
      "metadata": {
        "id": "5fjJ4zLRkr9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit a log-linear regression model to the training data       #nearly 2 time better than linear model\n",
        "Y_train_log = Y_train['Price'].transform(log)\n",
        "llm = sm.OLS(Y_train_log, X_train_const).fit()\n",
        "\n",
        "Y_pred_llm = np.exp(llm.predict(X_test_const))\n",
        "percent_errors = np.abs((Y_test['Price'] - Y_pred_llm) / Y_test['Price']) *100\n",
        "print(\"Log-Linear Model MAPE = \", np.mean(percent_errors), \"%\")"
      ],
      "metadata": {
        "id": "9Y7QhCCFkt_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit a log-log regression model to the training data\n",
        "X_train_log = X_train.copy()\n",
        "X_train_log['CaratWeight'] = X_train_log['CaratWeight'].transform(log)\n",
        "X_train_log_const = sm.add_constant(X_train_log)\n",
        "\n",
        "X_test_log = X_test.copy()\n",
        "X_test_log['CaratWeight'] = X_test_log['CaratWeight'].transform(log)\n",
        "X_test_log_const = sm.add_constant(X_test_log)\n",
        "\n",
        "\n",
        "loglog = sm.OLS(Y_train_log, X_train_log_const).fit()\n",
        "\n",
        "Y_pred_loglog = np.exp(loglog.predict(X_test_log_const))\n",
        "percent_errors = np.abs((Y_test['Price'] - Y_pred_loglog) / Y_test['Price']) *100\n",
        "print(\"Log-Log Model MAPE = \", np.mean(percent_errors), \"%\")"
      ],
      "metadata": {
        "id": "7QfraGIlkweE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}